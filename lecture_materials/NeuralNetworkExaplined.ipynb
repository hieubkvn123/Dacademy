{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Torch dependencies\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A basic neural network architecture consists of many artificial perceptrons :\n",
    "![ANN illustration](https://www.researchgate.net/profile/Facundo-Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a neural network ?\n",
    "* A neural network is a system of neurons that is responsible for the \"thinking\" made by human. An artificial neural network is a computation system that is inspired by the structure of human's neural network and used to replicate our decision making process. The basic unit of a neural network is an artificial perceptron.\n",
    "![Perceptron illustration](https://www.allaboutcircuits.com/uploads/articles/how-to-train-a-basic-perceptron-neural-network_rk_aac_image1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A simple neural network in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (input_layer): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (hidden_layer): Linear(in_features=8, out_features=2, bias=True)\n",
      "  (output_layer): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network as subclass of nn.Module\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, in_features, hidden=16, n_classes=10):\n",
    "        super(ANN, self).__init__()\n",
    "        self.input_layer = nn.Linear(in_features=in_features, out_features=hidden)\n",
    "        self.hidden_layer = nn.Linear(in_features=hidden, out_features=n_classes)\n",
    "        self.output_layer = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.input_layer(inputs)\n",
    "        outputs = self.hidden_layer(outputs)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "x = torch.normal(0, 1, size=(4, 12))\n",
    "model = ANN(12, hidden=8, n_classes=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation functions\n",
    "* If neural networks only consist of fully connected perceptrons activated by the dot product, the neural networks will become a linear model and would pretty much be useless learning non-linear patterns. To break this linearity nature, we have multiple activation functions to induce non-linearity:\n",
    "![Activation functions](https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png)\n",
    "\n",
    "* Some of the commonly used activation functions are :\n",
    "    - Rectified Linear Unet (ReLU) : Usually used in hidden layers or intermidiate layers. The reason why ReLU is used more favourably than Sigmoid and Tanh in hidden layers is because the gradient of ReLU is always constant when the input is non-negative. Hence, less likely to result in vanishing gradients.\n",
    "    - Sigmoid and Tanh : Usually used in the output layer in the case of **Binary classification**.\n",
    "    - Softmax : Usually used in the output layer in the case of **Multi-class classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNWithAct(\n",
      "  (input_layer): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (act_relu1): ReLU()\n",
      "  (hidden_layer): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (act_relu2): ReLU()\n",
      "  (output_layer): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network with activation functions in the constructor\n",
    "# You can copy this neural network architecture and use it in the previous session exercise to play with it abit\n",
    "class ANNWithAct(nn.Module):\n",
    "    def __init__(self, in_features, hidden=16, n_classes=10):\n",
    "        super(ANNWithAct, self).__init__()\n",
    "        self.input_layer = nn.Linear(in_features=in_features, out_features=hidden)\n",
    "        self.act_relu1 = nn.ReLU()\n",
    "    \n",
    "        \n",
    "        '''\n",
    "            Instead of nn.ReLU() for Rectified Linear Unit, you can use\n",
    "                - nn.Sigmoid() for sigmoid\n",
    "                - nn.Tanh() for tanh\n",
    "                - nn.Softmax() for softmax\n",
    "        '''\n",
    "        self.hidden_layer = nn.Linear(in_features=hidden, out_features=n_classes)\n",
    "        self.act_relu2 = nn.ReLU()\n",
    "        \n",
    "        self.output_layer = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.input_layer(inputs)\n",
    "        outputs = self.act_relu1(outputs)\n",
    "        outputs = self.hidden_layer(outputs)\n",
    "        outputs = self.act_relu2(outputs)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "model = ANNWithAct(16, hidden=8, n_classes=4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
